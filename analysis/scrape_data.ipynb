{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4970ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Season 1...\n",
      "Found 22 episodes in Season 1\n",
      "Scraping Season 2...\n",
      "Found 21 episodes in Season 2\n",
      "Scraping Season 3...\n",
      "Found 23 episodes in Season 3\n",
      "Scraping Season 4...\n",
      "Found 25 episodes in Season 4\n",
      "Scraping Season 5...\n",
      "Found 25 episodes in Season 5\n",
      "Scraping Season 6...\n",
      "Found 23 episodes in Season 6\n",
      "Scraping Season 7...\n",
      "Found 22 episodes in Season 7\n",
      "Scraping Season 8...\n",
      "Found 22 episodes in Season 8\n",
      "Scraping Season 9...\n",
      "Found 19 episodes in Season 9\n",
      "Scraping Season 10...\n",
      "Found 22 episodes in Season 10\n",
      "Scraping Season 11...\n",
      "Found 24 episodes in Season 11\n",
      "Scraping Season 12...\n",
      "Found 24 episodes in Season 12\n",
      "Scraping Season 13...\n",
      "Found 23 episodes in Season 13\n",
      "Scraping Season 14...\n",
      "Found 24 episodes in Season 14\n",
      "Scraping Season 15...\n",
      "Found 24 episodes in Season 15\n",
      "Scraping Season 16...\n",
      "Found 23 episodes in Season 16\n",
      "Scraping Season 17...\n",
      "Found 23 episodes in Season 17\n",
      "Scraping Season 18...\n",
      "Found 21 episodes in Season 18\n",
      "Scraping Season 19...\n",
      "Found 24 episodes in Season 19\n",
      "Scraping Season 20...\n",
      "Found 24 episodes in Season 20\n",
      "Scraping Season 21...\n",
      "Found 20 episodes in Season 21\n",
      "Scraping Season 22...\n",
      "Found 16 episodes in Season 22\n",
      "Scraping Season 23...\n",
      "Found 22 episodes in Season 23\n",
      "Scraping Season 24...\n",
      "Found 22 episodes in Season 24\n",
      "Scraping Season 25...\n",
      "Found 13 episodes in Season 25\n",
      "Scraping Season 26...\n",
      "Found 22 episodes in Season 26\n",
      "Scraping Season 27...\n",
      "Found 1 episodes in Season 27\n",
      "Scraping Season 28...\n",
      "Trying alternative selectors for Season 28...\n",
      "No episodes found for Season 28. This might be the last season.\n",
      "\n",
      "Found 574 episodes in total:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def scrape_svu_episodes():\n",
    "    base_url = \"https://www.imdb.com/title/tt0203259/episodes/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    }\n",
    "    \n",
    "    all_episodes = []\n",
    "    \n",
    "    # Loop through each season\n",
    "    season = 1\n",
    "    max_seasons = 40  # Set a reasonable upper limit\n",
    "    \n",
    "    while season <= max_seasons:\n",
    "        try:\n",
    "            print(f\"Scraping Season {season}...\")\n",
    "            season_url = f\"{base_url}?season={season}\"\n",
    "            response = requests.get(season_url, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Received status code {response.status_code}. Stopping.\")\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Check if we're on a valid season page\n",
    "            if \"Law\" not in soup.title.text:\n",
    "                print(f\"Season {season} does not exist. Stopping.\")\n",
    "                break\n",
    "                \n",
    "            # Find all episode items based on the HTML structure shared\n",
    "            episode_elements = soup.select('.episode-item-wrapper')\n",
    "            \n",
    "            if not episode_elements:\n",
    "                print(f\"Trying alternative selectors for Season {season}...\")\n",
    "                # Try different selectors based on IMDb's layout\n",
    "                episode_elements = soup.select('[data-testid=\"episodes-list\"] .ipc-metadata-list-summary-item') or \\\n",
    "                                  soup.select('.eplist .list_item') or \\\n",
    "                                  soup.select('.episodes-container .episode-container')\n",
    "            \n",
    "            if episode_elements:\n",
    "                for episode in episode_elements:\n",
    "                    # Extract title (now looking for the specific format in the provided HTML)\n",
    "                    title_elem = episode.select_one('.ipc-title__text')\n",
    "                    \n",
    "                    if not title_elem:\n",
    "                        title_elem = episode.select_one('a[itemprop=\"name\"]') or \\\n",
    "                                     episode.select_one('a.ipc-link--inherit-color') or \\\n",
    "                                     episode.select_one('a strong') or \\\n",
    "                                     episode.select_one('.title a')\n",
    "                    \n",
    "                    # Extract air date\n",
    "                    air_date_elem = episode.select_one('.larLSC') or episode.select_one('[class*=\"sc-\"] span')\n",
    "                    \n",
    "                    # Extract description\n",
    "                    desc_elem = episode.select_one('.ipc-html-content-inner-div') or episode.select_one('.item_description')\n",
    "                    \n",
    "                    # Extract rating\n",
    "                    rating_elem = episode.select_one('.ipc-rating-star--rating') or episode.select_one('.ipc-rating-star')\n",
    "                    \n",
    "                    # Extract image URL - NEW ADDITION\n",
    "                    image_elem = episode.select_one('.ipc-image')\n",
    "                    image_url = None\n",
    "                    if image_elem and image_elem.has_attr('src'):\n",
    "                        image_url = image_elem['src']\n",
    "                    \n",
    "                    # Process extracted data\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                        # Handle format like \"S1.E1 ∙ Payback\"\n",
    "                        if \"∙\" in title_text:\n",
    "                            title_text = title_text.split(\"∙\", 1)[1].strip()\n",
    "                        # Or handle format with episode number prefix\n",
    "                        elif \". \" in title_text and title_text[0].isdigit():\n",
    "                            title_text = title_text.split(\". \", 1)[1]\n",
    "                        \n",
    "                        # Create episode data dictionary\n",
    "                        episode_data = {\n",
    "                            \"Season\": season,\n",
    "                            \"Episode\": None,  # Will try to extract from title if possible\n",
    "                            \"Title\": title_text,\n",
    "                            \"Air Date\": air_date_elem.text.strip() if air_date_elem else None,\n",
    "                            \"Description\": desc_elem.text.strip() if desc_elem else None,\n",
    "                            \"Rating\": None,\n",
    "                            \"Image URL\": image_url  # Add the image URL\n",
    "                        }\n",
    "                        \n",
    "                        # Try to extract episode number if present in the format \"S1.E1\"\n",
    "                        if title_elem and \"S\" in title_elem.text and \".E\" in title_elem.text:\n",
    "                            ep_match = re.search(r'S\\d+\\.E(\\d+)', title_elem.text)\n",
    "                            if ep_match:\n",
    "                                episode_data[\"Episode\"] = int(ep_match.group(1))\n",
    "                        \n",
    "                        # Extract rating if present\n",
    "                        if rating_elem:\n",
    "                            rating_text = rating_elem.text.strip()\n",
    "                            rating_match = re.search(r'(\\d+\\.\\d+)', rating_text)\n",
    "                            if rating_match:\n",
    "                                episode_data[\"Rating\"] = float(rating_match.group(1))\n",
    "                        \n",
    "                        all_episodes.append(episode_data)\n",
    "                \n",
    "                print(f\"Found {len(episode_elements)} episodes in Season {season}\")\n",
    "                season += 1\n",
    "            else:\n",
    "                print(f\"No episodes found for Season {season}. This might be the last season.\")\n",
    "                break\n",
    "            \n",
    "            # Be respectful to the server\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Season {season}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_episodes\n",
    "\n",
    "# Scrape the episodes\n",
    "episodes = scrape_svu_episodes()\n",
    "\n",
    "# Convert to DataFrame for better display in Jupyter\n",
    "df_episodes = pd.DataFrame(episodes)\n",
    "\n",
    "# Display the results\n",
    "print(f\"\\nFound {len(episodes)} episodes in total:\")\n",
    "df_episodes.head(10)  # Show first 10 episodes\n",
    "\n",
    "# Save to CSV (optional)\n",
    "df_episodes.to_csv(\"output_data/law_and_order_svu_episodes.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
